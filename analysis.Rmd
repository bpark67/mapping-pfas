---
title: "Analysis for Statistical Mapping of PFOA and PFOS in Groundwater Throughout the Contiguous United States"
output: html_document
date: "2024-08-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(multispeciesPP)
library(randomForest)
```

```{r}
rm(list = ls())
```

# Data Processing

## Read Data

```{r read data}
df = read.csv("data/pfas data.csv")
ext = read.csv("data/extrapolation data.csv")
```

### Designate Presence-Absence Data States

2 observations per 1000 square miles && 1 observation within each 1 degree X 1 degree pixel

```{r designate pa states}
pa_states = c("CA", "WI", "WV", "OH", "NC", "SC", "MA", "NJ", "CO", "RI", "MD", "MI", "NH")
```

### Set ppt Detection Threshold

8 parts per trillion

```{r designate detection threshold}
df$PFAS8 = ifelse(df$Concentration >= 8, 1, 0)
```

### Create Violin Plot Visualizations

```{r create violin}
# Violin Plot
pviol = df %>%
  mutate(PA = ifelse(STUSPS %in% pa_states, "PA", "PO")) %>%
  ggplot(aes(x = PA, y = log(Concentration + 1), fill = PA)) +
  geom_violin() +
  geom_boxplot(width = 0.1, fill = "white") +
  geom_hline(yintercept = log(8 + 1), color = "red") +
  theme_bw() +
  labs(x = "Presence-Absence",
       y = "Log(PFAS Concentration + 1)") +
  annotate("text", x = 1.5, y = log(8 + 1) + 0.5, label = "log(8 + 1)",
           color = "red", size = 5) +
  theme(legend.position = "none",
        axis.title.x = element_text(size = 15),
        axis.title.y = element_text(size = 15),
        axis.text.x = element_text(size = 13),
        axis.text.y = element_text(size = 13)) +
  scale_fill_manual(values = c("#003366", "#006400"))

# ggsave("figures/svg/PFASConcentrationViolin.eps", pviol, width = 8, height = 6)
```

### Create Summary Table (in LaTeX)

```{r}
summary_statistics = matrix(nrow = ncol(df)-4, ncol = 5)

for(i in 1:(ncol(df)-4)){
  summary_statistics[i,] = c(mean(df[[i]]), sd(df[[i]]), min(df[[i]]), median(df[[i]]), max(df[[i]]))
}

row.names(summary_statistics) = colnames(df)[1:16]
colnames(summary_statistics) = c("Mean", "SD", "Min", "Median", "Max")

summary_statistics %>%
  round(2) %>%
  as.data.frame() %>%
  knitr::kable("latex", booktabs = T, caption = "Summary Statistics of Data Set")
```



## Simulate Skewedness; Remove Lower 50%

```{r splitting by thresholds}
thresh = c(1, 0.5)

n_thresh = round(nrow(df) * thresh)

df_list = list()

for (i in 1:length(n_thresh)){
  # Data is already arranged in descending order
  # Pull out only the first x% of rows
  df_list[[i]] = df[1:n_thresh[i],]
}
```

## Set Target Covariates

```{r set target columns}
covariates = colnames(df)[! colnames(df) %in% c("Latitude", 
                                                "Longitude", 
                                                "Concentration", 
                                                "STUSPS", 
                                                "PFAS8")]
```

## Split into PO and PA Data

```{r}
df_polist = list()
df_palist = list()

set.seed(16)
for(i in 1:length(n_thresh)){
# Choose rows for PA
df_palist[[i]] = df_list[[i]] %>%
  filter(df_list[[i]]$STUSPS %in% pa_states)

# Choose rows for PO (not in PA)
df_polist[[i]] = df_list[[i]] %>%
  filter(!df_list[[i]]$STUSPS %in% pa_states)
}
```

## Create Full Background (for PO Data)

```{r}
combined_list = list()

for(i in 1:length(n_thresh)){
combined_list[[i]] = df_polist[[i]] %>%
  select(any_of(covariates)) %>%
  bind_rows(ext %>% select(any_of(covariates)))
}
```

# Fit IPP Model

```{r}
mod_list = list()
intensity_formula = as.formula(paste("~", paste(covariates, collapse = "+")))
bias_formula = as.formula(paste("~ ", paste(covariates, collapse = "+")))
#################### Prepare the covariates ####################################
# Covariates for intensity
x = df %>%
  select(-c("PFAS8")) %>%
  select(any_of(covariates)) %>%
  mutate_all(as.numeric)
# Covariates for bias
z = df %>%
  select(-c("PFAS8"))%>%
  select(any_of(covariates)) %>%
  mutate_all(as.numeric)

#################### START LOOP HERE ###########################################
for(i in 1:length(n_thresh)){
#################### Background Data ###########################################
# Combine PO Data + Extrapolation points
BG = cbind(combined_list[[i]] %>%
             select(any_of(covariates)) %>%
             mutate_all(as.numeric),
           combined_list[[i]] %>%
             select(any_of(covariates)) %>%
             mutate_all(as.numeric))

#################### Presence-only Data ########################################
# All "Presence" rows from PO Data
PO_rows = row.names(df_polist[[i]][df_polist[[i]]["PFAS8"] == 1,])

# Grab all points of presences from Master DF
PO = data.frame(x, z)[PO_rows,]

# Turn into a list
PO.list = list(a = PO)

# Remove for memory
rm(PO, PO_rows)

#################### Presence-Absence Data #####################################
# Presence-Absence column from PA data
a = df_palist[[i]]$PFAS8

# Rows of the PA dataset
PA_rows = row.names(df_palist[[i]])
PA = data.frame(x[PA_rows,], a)

rm(a, PA_rows)
#################### Fit IPP Model #############################################
mod_list[[i]] = multispeciesPP(intensity_formula,
                               bias_formula,
                               PA,
                               PO.list,
                               BG,
                               region.size = nrow(BG),
                               penalty.l2.sdm=.00001,
                               penalty.l2.bias=.001,
                               penalty.l2.intercept=0.0001,
                               control=list(trace=TRUE,maxit=100))
}
```

## Conduct Covariate Analysis

```{r}
se100 = mod_list[[1]]$std.errs[1:17]
se50 = mod_list[[2]]$std.errs[1:17]
names(se100) <- sub("^a:", "", names(se100))
names(se50) <- sub("^a:", "", names(se50))

estimates = data.frame(
  "100 Intensity" = mod_list[[1]]$species.coef,
  "100 SE" = se100,
  "50 Intensity" = mod_list[[2]]$species.coef,
  "50 SE" = se50
)

intercept = signif(estimates[1,], 3)

estimates = estimates[-1,]

cov_an = estimates %>%
  signif(3) %>%
  arrange(desc(abs(a))) %>%
  tibble::add_row(a = intercept$a, 
                  a.1 = intercept$a.1, 
                  X100.SE = intercept$X100.SE,
                  X50.SE = intercept$X50.SE,
                  .before = 1)

row.names(cov_an) = c("(Intercept)", row.names(cov_an)[-1])

cov_an %>%
  knitr::kable("latex", booktabs = T, caption = "Covariate Analysis of Intensity from IPP Model using 100% and 50% of Data") %>%
  kableExtra::add_header_above(c(" " = 1, "100% Data" = 2, "50% Data" = 2))
```

```{r}
bias_estimates = data.frame(
  "b100" = mod_list[[1]]$bias.coef,
  "b50" = mod_list[[2]]$bias.coef
) %>% signif(3) %>%
  arrange(desc(abs(b100)))

bias_estimates %>%
  knitr::kable("latex", booktabs = T, caption = "Bias Covariate Analysis of IPP Model using 100% and 50% of Data") %>%
  kableExtra::add_header_above(c(" " = 1, "100% Data" = 1, "50% Data" = 1))
```

## Compare Two Models

```{r}
compare = data.frame(
  m1 = mod_list[[1]]$fit.BG[(nrow(df_polist[[1]]) + 1):nrow(mod_list[[1]]$fit.BG)],
  m2 = mod_list[[2]]$fit.BG[(nrow(df_polist[[2]]) + 1):nrow(mod_list[[2]]$fit.BG)]
)

compare$delta = compare$m1 - compare$m2

ipp1 = compare %>%
  ggplot(aes(x = log(m1), y = log(m2))) +
  geom_point() +
  theme_bw() +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  annotate("text", x = 0, y = -2, label = "45° Line", size = 7, col = "red", angle = 35) +
  labs(x = latex2exp::TeX("$log(\\lambda_{100\\%})$"), 
       y = latex2exp::TeX("$log(\\lambda_{50\\%})$")) +
  theme(axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20)) # Increase size of x lab and ylab

# ggsave("figures/svg/IPPComparison.eps", ipp1, width = 8, height = 6)
```


## Save

### Intensity
```{r}
intensity_datasets = list()

for(i in 1:length(n_thresh)){
  # NOTE: We exclude .fit.PA because it estimates Probability, not Intensity
intensity_datasets[[i]] = bind_rows(df_polist[[i]][c("Longitude", "Latitude")],
                                    ext[c("Longitude", "Latitude")])

intensity_datasets[[i]]$fit = as.vector(mod_list[[i]]$fit.BG)
}

# write.csv(intensity_datasets[[1]],
#           "results/100% Intensity Predictions.csv",
#           row.names = F)
# write.csv(intensity_datasets[[2]],
#           "Results/50% Intensity Predictions.csv",
#           row.names = F)
```

### Bias

```{r}
bias_datasets = list()

for(i in 1:length(n_thresh)){
bias_datasets[[i]] = bind_rows(df_polist[[i]][c("Longitude", "Latitude")], 
                               ext[c("Longitude", "Latitude")])

bias_datasets[[i]]$bias = as.vector(mod_list[[i]]$bias.fit.BG)
}


# write.csv(bias_datasets[[1]],
#           "results/100% Bias Function.csv",
#           row.names = F)
# write.csv(bias_datasets[[2]],
#           "results/50% Bias Function.csv",
#           row.names = F)
```


# Fit Random Forest

```{r}
rf_list = list()

rf_formula = as.formula(paste("as.factor(PFAS8)~", paste(covariates, collapse = "+")))

set.seed(16)

for(i in 1:2){
rf_list[[i]] = randomForest(rf_formula, 
                            data = df_list[[i]], 
                            proximity = T,
                            importance = T,
                            keep.forest = T)
}
```

## Confusion Matrix/Sensitivity/Specificity

```{r}
rf_list[[1]]$confusion
rf_list[[2]]$confusion
```

## Covariate Analysis

```{r}
mean_decrease_accuracy = data.frame(
  "Importance100" = rf_list[[1]]$importance[,3],
  "Importance50" = rf_list[[2]]$importance[,3]
) %>% round(3) %>%
  arrange(desc(abs(Importance100)))

mean_decrease_accuracy %>%
  knitr::kable("latex", booktabs = T, caption = "Covariate Analysis of Random Forest Model using 100% and 50% of Data")
```


## Extrapolate

```{r extrapolated predictions}
ext_preds = list()

for (i in 1:length(n_thresh)){
pred = predict(rf_list[[i]],
               newdata = rbind(df_list[[i]][covariates], ext[covariates]),
               type = "prob")

locs = rbind(df_list[[i]][c("Longitude", "Latitude")],
             ext[c("Longitude", "Latitude")])

locs$fit = unname(pred[,2])

ext_preds[[i]] = locs
}
```

## Compare Two Models

```{r}
compare_rf = data.frame(
  m1 = ext_preds[[1]]$fit[(nrow(df_list[[1]]) + 1):nrow(ext_preds[[1]])],
  m2 = ext_preds[[2]]$fit[(nrow(df_list[[2]]) + 1):nrow(ext_preds[[2]])]
)

compare_rf$delta = compare$m1 - compare$m2

rf1 = compare_rf %>%
  ggplot(aes(x = log(m1), y = log(m2))) +
  geom_point() +
  theme_bw() +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  annotate("text", x = 0, y = -0.2, label = "45° Line", size = 7, col = "red", angle = 70) +
  labs(x = latex2exp::TeX("$log(p_{100\\%})$"),
       y = latex2exp::TeX("$log(p_{50\\%}$)")) +
  theme(axis.title.x = element_text(size = 20),
        axis.title.y = element_text(size = 20))

# ggsave("figures/svg/RFComparison.eps", rf1, width = 8, height = 6)
```


## Save

```{r}
# write.csv(ext_preds[[1]], "results/100% RF Predictions.csv", row.names = F)
# write.csv(ext_preds[[2]], "results/50% RF Predictions.csv", row.names = F)
```

# Kriging

## 100 %

```{r}
library(sp)
library(gstat)

spdf = SpatialPointsDataFrame(coords = df[,c("Longitude", "Latitude")],
                              data = df,
                              proj4string = CRS("+proj=longlat +datum=WGS84"))
spdf$logConcentration = log(spdf$Concentration + 1)

spext = SpatialPointsDataFrame(coords = ext[,c("Longitude", "Latitude")],
                               data = ext,
                               proj4string = CRS("+proj=longlat +datum=WGS84"))

vgm = variogram(logConcentration ~ 1, spdf)
fit.vgm = fit.variogram(vgm, vgm(model = "Mat"))

plot(vgm, fit.vgm)

# spext %>%
#   as.data.frame() %>%
#   ggplot(aes(x = Longitude, y = Latitude)) +
#   geom_point() +
#   theme_bw()

kriged = krige(logConcentration ~ 1, spdf, spext, fit.vgm)

write.csv(cbind(kriged@data, kriged@coords), "results/Kriging.csv", row.names = F)
```

## 50%

```{r}
spdf = SpatialPointsDataFrame(coords = df_list[[2]][,c("Longitude", "Latitude")],
                              data = df_list[[2]],
                              proj4string = CRS("+proj=longlat +datum=WGS84"))

spdf$logConcentration = log(spdf$Concentration + 1)

vgm = variogram(logConcentration ~ 1, spdf)
fit.vgm = fit.variogram(vgm, vgm(model = "Mat"))

plot(vgm, fit.vgm)

kriged = krige(logConcentration ~ 1, spdf, spext, fit.vgm)

write.csv(cbind(kriged@data, kriged@coords), "results/Kriging50.csv", row.names = F)
```




